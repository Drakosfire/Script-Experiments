import os
from pydantic import Field
from typing import List, Mapping, Optional, Any
from langchain.llms.base import LLM
import torch
import time
import random
from exllamav2 import(
    ExLlamaV2,
    ExLlamaV2Config,
    ExLlamaV2Cache,
    ExLlamaV2Tokenizer,
)

from exllamav2.generator import (
    ExLlamaV2BaseGenerator,
    ExLlamaV2Sampler
)

class MyExllamav2(LLM):
    """
    A custom LLM class that integrates ExLlamav2 models
    
    Arguments:

    model_folder_path: (str) Folder path where the model lies
    model_name: (str) The name of the model to use (<model name>.bin)
    allow_download: (bool) whether to download the model or not

    backend: (str) The backend of the model (Supported backends: llama/gptj)
    n_threads: (str) The number of threads to use
    n_predict: (str) The maximum numbers of tokens to generate
    temp: (str) Temperature to use for sampling
    top_p: (float) The top-p value to use for sampling
    top_k: (float) The top k values use for sampling
    n_batch: (int) Batch size for prompt processing
    repeat_last_n: (int) Last n number of tokens to penalize
    repeat_penalty: (float) The penalty to apply repeated tokens
    
    """
    # First value in a field is the default value, so I've changed it from None to the folder with the model
    
    model_folder_path: str = Field( None, alias='model_folder_path')
    #model_name: str = Field(None, alias='model_name')
    allow_download: bool = Field(None, alias='allow_download')
    config: ExLlamaV2Config = Field(default_factory=ExLlamaV2Config)


    
    

    # initialize the model
    ExLlamaV2_model_instance:Any = None 

    def __init__(self, model_folder_path, **kwargs):
        # super here acts to call the Class as it's self?
        super(MyExllamav2, self).__init__()
        self.model_folder_path: str = model_folder_path
        # This creates an instance of ExLlamav2 passing in the folder path.

        config = ExLlamaV2Config()
        print(config)
        config.model_dir = model_folder_path
        print(config)
        config.prepare()
        print(config)
        self.config = config
        print(config)
        print(self.config)
        
        model = ExLlamaV2(self.config)
              

        cache = ExLlamaV2Cache(model, lazy = True)
        model.load_autosplit(cache)
        print(model)

        tokenizer = ExLlamaV2Tokenizer(config)   
        # Initialize generator

        generator = ExLlamaV2BaseGenerator(model, cache, tokenizer)

         
        
        generator.warmup()
        
        self.ExLlamaV2_model_instance = generator
        
         
    @property
    def _get_model_default_parameters(self):
        return {
            "max_tokens": self.max_tokens,
            "n_predict": self.n_predict,
            "top_k": self.top_k,
            "top_p": self.top_p,
            "temp": self.temp,
            "n_batch": self.n_batch,
            "repeat_penalty": self.repeat_penalty,
            "repeat_last_n": self.repeat_last_n,
        }

    @property
    def _identifying_params(self) -> Mapping[str, Any]:
        """
        Get all the identifying parameters
        """
        return {
            #'model_name' : self.model_name,
            'model_path' : self.model_folder_path,
            'model_parameters': self._get_model_default_parameters
        }

    @property
    def _llm_type(self) -> str:
        return 'llama'
    
    def _call(self, prompt: str, stop: Optional[List[str]] = None, **kwargs) -> str:
        """
        Args:
            prompt: The prompt to pass into the model.
            stop: A list of strings to stop generation when encountered

        Returns:
            The string generated by the model        
        """
        

        print("Loading model: " + self.model_folder_path)
        #print(f"Model load took {time.time() - start_time}")
        print(f"Memory allocated : {torch.cuda.memory_allocated()}")  
        
        # Define settings

        settings = ExLlamaV2Sampler.Settings()
        settings.temperature = 0.7
        settings.top_k = 40
        settings.top_p = 0.95
        settings.token_repetition_penalty = 1.1
        max_new_tokens = 2000
        

        response = self.ExLlamaV2_model_instance.generate_simple(prompt, settings, max_new_tokens, seed = random.randint(0,10**20))
        print(response)
        return response
